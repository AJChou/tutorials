---
layout: tutorial
title: Real Time Data Transportation and Ingestion
tutorial-id: 220
tutorial-series: Streaming
tutorial-version: hdp-2.4.0
intro-page: true
components: [ nifi, storm, kafka ]
---


# Real-time Data transportation and Ingestion

## Introduction

Welcome to a three part tutorial series on real-time data ingesting and analysis.  The speed of today's processing systems have moved from classical data warehousing batch reporting to the realm of real-time processing and analytics. The result is real-time business intelligence. Real-time means near to zero latency and access to information whenever it is required. This tutorial will show how geolocation information from trucks can be combined with sensor data from trucks and roads.  These sensors report real-time events like speeding, lane-departure, unsafe tailgating, and unsafe following distances. We will capture these events in real-time.

## Pre-Requisites

*  Downloaded and Installed latest [Hortonworks Sandbox](http://hortonworks.com/products/hortonworks-sandbox/#install)
*  [Learning the Ropes of the Hortonworks Sandbox](http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/)
*   8GB+ RAM (Assigning more is recommended) and preferably 4 processor cores, otherwise you may encounter errors in the third tutorial
*   Data sets used:
  *   New York City Truck Routes from NYC DOT.
  *   Truck Events Data generated using a custom simulator.
  *   Weather Data, collected using APIs from Forcast.io.
  *   Traffic Data, collected using APIs from MapQuest.

All data sets used in these tutorials are real data sets but modified to fit these use cases

## Tutorial Overview

The events generated by sensors will be ingested and routed by Apache NiFi, captured through a distributed publish-subscribe messaging system named Apache Kafka. We will use Apache Storm to process this data from Kafka and eventually persist that data into HDFS and HBase.

## Goals of the tutorial

*   Understand Real-time Data Analysis
*   Understand Apache NiFi Architecture
*   Create NiFi DataFlow
*   Understand Apache Kafka Architecture
*   Create Consumers in Kafka
*   Understand Apache Storm Architecture
*   Create Spouts and Bolts in Storm
*   Persist data from Storm into Hive and Hbase

## Outline

1.  Tutorial Introduction
2.  Pre-Requisites:
  -  Data sets used described above
  -  Downloaded and Installed latest [Hortonworks Sandbox](http://hortonworks.com/products/hortonworks-sandbox/#install)
  -  [Learning the Ropes of the Hortonworks Sandbox](http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/)
3.  Tutorial Overview
4.  Goals of the Tutorial
5.  Concepts:
  -  [Apache NiFi](rtep-concepts.md)
  -  [Apache NiFi on Kafka](rtep-concepts.md)
  -  [Apache Kafka](rtep-concepts.md)
  -  [Apache Storm](rtep-concepts.md)
  -  [Apache Kafka on Storm](rtep-concepts.md)
  -  [Hortonworks DataFlow](tep-concepts.md)
6.  Get Started with HDF labs:
  - Lab 0: [Apache NiFi: Ingest, Filter and Land Real-Time Event Stream](rtep-1.md)
  - Lab 1: [Apache Kafka: Real-time event stream transportation](rtep-1.md)
  - Lab 2: [Ingest Real-Time Data into HBase & Hive using Storm](rtep-2.md)
7.  Next Steps/Try these
8.  References
  -  [Apache NiFi Documentation](https://nifi.apache.org/docs.html)
  -  [Apache Kafka Documentation](http://kafka.apache.org/documentation.html)
  -  [Apache Storm Documentation](http://storm.apache.org/releases/1.0.0/index.html)
  -  [Hortonworks DataFlow](http://hortonworks.com/products/hdf/)
